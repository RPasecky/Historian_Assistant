{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "from datetime import date, datetime\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Ensure notebooks can import project modules.\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from Models import artifact_db\n",
    "from Models.artifact_models import (\n",
    "    Artifact,\n",
    "    DocumentExtraction,\n",
    "    Person,\n",
    "    Location,\n",
    "    ContextChunk,\n",
    "    Event,\n",
    "    Milestone,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database initialized: /Users/dirk/Historian_Assistant/Models/data/historian.db\n",
      "Reading from: /Users/dirk/Historian_Assistant/Library/data.txt\n"
     ]
    }
   ],
   "source": [
    "# Configure the artifact source and metadata.\n",
    "ARTIFACT_FILE = Path(\"../Library/data.txt\")\n",
    "MAX_CHARS = 8000  # trim super long sources for notebook experimentation\n",
    "\n",
    "ARTIFACT_TITLE = \"Nikola Tesla - Test Manuscript\"\n",
    "ARTIFACT_AUTHOR = \"Nikola Tesla\"\n",
    "\n",
    "artifact_db.init_db()\n",
    "artifact_metadata = Artifact(title=ARTIFACT_TITLE, author=ARTIFACT_AUTHOR)\n",
    "print(f\"Reading from: {ARTIFACT_FILE.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_JSON = json.dumps(DocumentExtraction.model_json_schema(), indent=2)\n",
    "DOCUMENT_EXTRACTION_INSTRUCTIONS = \"\"\"You are a historian data assistant. Read the artifact text and extract structured data.\n",
    "Please follow these guidelines:\n",
    "- Capture a concise artifact summary plus key people, locations (with address + coordinates when present), events, and milestones.\n",
    "- Map page numbers to `[start, end]` ranges where possible.\n",
    "- Use ISO dates (`YYYY-MM-DD`) when you can infer an exact day. Otherwise provide the most precise partial you can (e.g., `1919`, `1903-05`).\n",
    "- Person and location names should stay consistent so they can be re-linked later.\n",
    "- Only include milestone or event participants when the text clearly states their involvement.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_artifact_text(path: Path, max_chars: int) -> str:\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    return text[:max_chars]\n",
    "\n",
    "\n",
    "def build_prompt(artifact: Artifact, text: str) -> str:\n",
    "    return f\"\"\"{DOCUMENT_EXTRACTION_INSTRUCTIONS}\n",
    "\n",
    "Artifact metadata:\n",
    "{json.dumps(artifact.model_dump(), indent=2, default=str)}\n",
    "\n",
    "Target schema:\n",
    "{SCHEMA_JSON}\n",
    "\n",
    "Artifact text:\n",
    "{text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    "    max_retries=2,\n",
    ")\n",
    "structured_llm = llm.with_structured_output(DocumentExtraction)\n",
    "print(\"LLM ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentExtraction(artifact=Artifact(id=UUID('ad4aba09-ea20-488b-be61-7b26b702f692'), title='Nikola Tesla - Test Manuscript', author='Nikola Tesla', publication_year=None, time_period_start=None, time_period_end=None, created_at=datetime.datetime(2025, 11, 29, 15, 43, 29, 524734)), persons=[PersonExtraction(name='Nikola Tesla', aliases=[], birth_year=None, death_year=None), PersonExtraction(name='William B. Rankine', aliases=[], birth_year=None, death_year=None), PersonExtraction(name='Mr. Morgan', aliases=[], birth_year=None, death_year=None), PersonExtraction(name='Colonel Astor', aliases=[], birth_year=None, death_year=None)], locations=[], context_chunks=[ContextChunkExtraction(chunk_label='Professional Announcement', page_range=[], summary='Nikola Tesla announces his availability for professional services as a consulting electrician and engineer, anticipating revolutionary advancements in various industries through high-potential and high-frequency currents.', key_persons=['Nikola Tesla'], key_locations=['New York City', 'Long Island']), ContextChunkExtraction(chunk_label='Wireless Transmission - Early Lectures', page_range=[], summary=\"Excerpts from Tesla's 1893 lectures before the Franklin Institute and National Electric Light Association, discussing the practicability of wireless transmission of intelligible signals and power, emphasizing the use of the earth as a medium.\", key_persons=['Nikola Tesla'], key_locations=['Philadelphia', 'St. Louis']), ContextChunkExtraction(chunk_label='Wireless Transmission - Stationary Waves', page_range=[], summary=\"Excerpts from Tesla's 1900 article detailing the production of powerful electrical effects and the absolute certainty of global wireless communication through the discovery of 'stationary' electrical waves, referencing U.S. Patents 645,576 and 649,621.\", key_persons=['Nikola Tesla'], key_locations=[]), ContextChunkExtraction(chunk_label='Correspondence with William B. Rankine', page_range=[], summary='A letter from Nikola Tesla to William B. Rankine dated April 19, 1904, discussing the financial status of The Nikola Tesla Company, patent agreements with Mr. Morgan and Colonel Astor, and the progress of the Wardenclyffe plant for global telegraphy and telephony.', key_persons=['Nikola Tesla', 'William B. Rankine', 'Mr. Morgan', 'Colonel Astor'], key_locations=['New York City', 'Wardenclyffe', 'Long Island'])], events=[EventExtraction(description='Nikola Tesla announces his professional services as a consulting electrician and engineer.', page_range=[], event_type=None, context_label=None, event_date='1904-01-01', quotes=[], interactions=[], person_names=['Nikola Tesla'], location_names=['New York City', 'Long Island']), EventExtraction(description=\"Nikola Tesla delivers a lecture 'Light and Other High-frequency Phenomena' before the Franklin Institute.\", page_range=[], event_type=None, context_label=None, event_date='1893-02', quotes=[], interactions=[], person_names=['Nikola Tesla'], location_names=['Philadelphia']), EventExtraction(description=\"Nikola Tesla delivers a lecture 'Light and Other High-frequency Phenomena' before the National Electric Light Association.\", page_range=[], event_type=None, context_label=None, event_date='1893-03', quotes=[], interactions=[], person_names=['Nikola Tesla'], location_names=['St. Louis']), EventExtraction(description=\"Nikola Tesla publishes 'The Problem of Increasing Human Energy' in Century magazine.\", page_range=[], event_type=None, context_label=None, event_date='1900-06', quotes=[], interactions=[], person_names=['Nikola Tesla'], location_names=[]), EventExtraction(description='Nikola Tesla made a personal agreement with Mr. Morgan regarding the assignment of patents related to telegraphy and lighting.', page_range=[], event_type=None, context_label=None, event_date='prior to 1904-04-19', quotes=[], interactions=[], person_names=['Nikola Tesla', 'Mr. Morgan'], location_names=[]), EventExtraction(description=\"Mr. Morgan accepted Nikola Tesla's proposal to join in all his inventions.\", page_range=[], event_type=None, context_label=None, event_date='prior to 1904-04-19', quotes=[], interactions=[], person_names=['Nikola Tesla', 'Mr. Morgan'], location_names=[]), EventExtraction(description=\"Colonel Astor's interest in Nikola Tesla's inventions was adjusted.\", page_range=[], event_type=None, context_label=None, event_date='prior to 1904-04-19', quotes=[], interactions=[], person_names=['Colonel Astor'], location_names=[]), EventExtraction(description=\"An attempt was made to form 'Tesla Electric & Manufacturing Company' with a capital of $5,000,000.\", page_range=[], event_type=None, context_label=None, event_date='1903', quotes=[], interactions=[], person_names=['Nikola Tesla'], location_names=[]), EventExtraction(description='Nikola Tesla writes a letter to William B. Rankine.', page_range=[], event_type=None, context_label=None, event_date='1904-04-19', quotes=[], interactions=[], person_names=['Nikola Tesla', 'William B. Rankine'], location_names=['New York City'])], milestones=[MilestoneExtraction(person_name='Nikola Tesla', milestone_type='Professional Announcement', milestone_date='1904-01-01', description='Announced professional services as a consulting electrician and engineer.', location_name='New York City'), MilestoneExtraction(person_name='Nikola Tesla', milestone_type='Lecture', milestone_date='1893-02', description=\"Delivered 'Light and Other High-frequency Phenomena' before the Franklin Institute.\", location_name='Philadelphia'), MilestoneExtraction(person_name='Nikola Tesla', milestone_type='Lecture', milestone_date='1893-03', description=\"Delivered 'Light and Other High-frequency Phenomena' before the National Electric Light Association.\", location_name='St. Louis'), MilestoneExtraction(person_name='Nikola Tesla', milestone_type='Publication', milestone_date='1900-06', description=\"Published 'The Problem of Increasing Human Energy' in Century magazine.\", location_name=None), MilestoneExtraction(person_name='Nikola Tesla', milestone_type='Discovery', milestone_date='prior to 1900-06', description=\"Discovered 'stationary' electrical waves, enabling wireless communication and power transmission.\", location_name=None), MilestoneExtraction(person_name='Nikola Tesla', milestone_type='Business Agreement', milestone_date='prior to 1904-04-19', description='Made a personal agreement with Mr. Morgan regarding patent assignments.', location_name=None), MilestoneExtraction(person_name='Nikola Tesla', milestone_type='Business Venture', milestone_date='1903', description=\"Attempted to form 'Tesla Electric & Manufacturing Company'.\", location_name=None), MilestoneExtraction(person_name='Nikola Tesla', milestone_type='Project Development', milestone_date='1904-07', description='Construction of the Wardenclyffe plant.', location_name='Wardenclyffe, Long Island')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = load_artifact_text(ARTIFACT_FILE, MAX_CHARS)\n",
    "prompt = build_prompt(artifact_metadata, raw_text)\n",
    "extraction = structured_llm.invoke(prompt)\n",
    "# enforce trusted artifact metadata so downstream IDs remain stable\n",
    "extraction = extraction.model_copy(update={\"artifact\": artifact_metadata})\n",
    "extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact: Nikola Tesla - Test Manuscript (Nikola Tesla)\n",
      "People: 4 | Locations: 0 | Events: 9 | Milestones: 8\n",
      "Context chunks: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Artifact: {extraction.artifact.title} ({extraction.artifact.author})\")\n",
    "print(f\"People: {len(extraction.persons)} | Locations: {len(extraction.locations)} | Events: {len(extraction.events)} | Milestones: {len(extraction.milestones)}\")\n",
    "if extraction.context_chunks:\n",
    "    print(f\"Context chunks: {len(extraction.context_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json_or_null(value):\n",
    "    if value in (None, [], {}):\n",
    "        return None\n",
    "    return json.dumps(value)\n",
    "\n",
    "\n",
    "def _maybe_date(value: str | None):\n",
    "    if not value:\n",
    "        return None\n",
    "    cleaned = value.strip()\n",
    "    if not cleaned:\n",
    "        return None\n",
    "    normalized = cleaned.replace(\"XX\", \"01\")\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y-%m\", \"%Y\"):\n",
    "        try:\n",
    "            parsed = datetime.strptime(normalized, fmt)\n",
    "            if fmt == \"%Y\":\n",
    "                return date(parsed.year, 1, 1)\n",
    "            if fmt == \"%Y-%m\":\n",
    "                return date(parsed.year, parsed.month, 1)\n",
    "            return parsed.date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    match = re.search(r\"(\\d{4})\", cleaned)\n",
    "    if match:\n",
    "        return date(int(match.group(1)), 1, 1)\n",
    "    return None\n",
    "\n",
    "def persist_document(extraction: DocumentExtraction) -> Dict[str, int]:\n",
    "    conn = artifact_db.get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    artifact = extraction.artifact\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT OR REPLACE INTO artifacts (id, title, author, publication_year, time_period_start, time_period_end, created_at)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (\n",
    "            str(artifact.id),\n",
    "            artifact.title,\n",
    "            artifact.author,\n",
    "            artifact.publication_year,\n",
    "            artifact.time_period_start,\n",
    "            artifact.time_period_end,\n",
    "            artifact.created_at.isoformat(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    person_lookup: Dict[str, str] = {}\n",
    "    for person_data in extraction.persons:\n",
    "        key = person_data.name.strip().lower()\n",
    "        if not key or key in person_lookup:\n",
    "            continue\n",
    "        person = Person(\n",
    "            name=person_data.name,\n",
    "            aliases=person_data.aliases,\n",
    "            artifact_id=artifact.id,\n",
    "            birth_year=person_data.birth_year,\n",
    "            death_year=person_data.death_year,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO persons (id, name, aliases, artifact_id, birth_year, death_year, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(person.id),\n",
    "                person.name,\n",
    "                _json_or_null(person.aliases),\n",
    "                str(person.artifact_id),\n",
    "                person.birth_year,\n",
    "                person.death_year,\n",
    "                person.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        person_lookup[key] = str(person.id)\n",
    "\n",
    "    location_lookup: Dict[str, str] = {}\n",
    "    for location_data in extraction.locations:\n",
    "        key = location_data.name.strip().lower()\n",
    "        if not key or key in location_lookup:\n",
    "            continue\n",
    "        location = Location(\n",
    "            name=location_data.name,\n",
    "            aliases=location_data.aliases,\n",
    "            artifact_id=artifact.id,\n",
    "            address=location_data.address,\n",
    "            latitude=location_data.latitude,\n",
    "            longitude=location_data.longitude,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO locations (id, name, aliases, artifact_id, address, latitude, longitude, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(location.id),\n",
    "                location.name,\n",
    "                _json_or_null(location.aliases),\n",
    "                str(location.artifact_id),\n",
    "                location.address,\n",
    "                location.latitude,\n",
    "                location.longitude,\n",
    "                location.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        location_lookup[key] = str(location.id)\n",
    "\n",
    "    chunk_lookup: Dict[str, str] = {}\n",
    "    for chunk_data in extraction.context_chunks:\n",
    "        chunk = ContextChunk(\n",
    "            artifact_id=artifact.id,\n",
    "            chunk_label=chunk_data.chunk_label,\n",
    "            page_range=chunk_data.page_range,\n",
    "            summary=chunk_data.summary,\n",
    "            key_persons=chunk_data.key_persons,\n",
    "            key_locations=chunk_data.key_locations,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO context_chunks (id, artifact_id, chunk_label, page_range, summary, key_persons, key_locations, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(chunk.id),\n",
    "                str(chunk.artifact_id),\n",
    "                chunk.chunk_label,\n",
    "                _json_or_null(chunk.page_range),\n",
    "                chunk.summary,\n",
    "                _json_or_null(chunk.key_persons),\n",
    "                _json_or_null(chunk.key_locations),\n",
    "                chunk.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        if chunk.chunk_label:\n",
    "            chunk_lookup[chunk.chunk_label.strip().lower()] = str(chunk.id)\n",
    "\n",
    "    for event_data in extraction.events:\n",
    "        context_chunk_id = None\n",
    "        if event_data.context_label:\n",
    "            context_chunk_id = chunk_lookup.get(event_data.context_label.strip().lower())\n",
    "        event = Event(\n",
    "            description=event_data.description,\n",
    "            artifact_id=artifact.id,\n",
    "            page_range=event_data.page_range,\n",
    "            context_chunk_id=context_chunk_id,\n",
    "            event_type=event_data.event_type,\n",
    "            event_date=_maybe_date(event_data.event_date),\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO events (id, description, artifact_id, page_range, context_chunk_id, event_type, event_date, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(event.id),\n",
    "                event.description,\n",
    "                str(event.artifact_id),\n",
    "                _json_or_null(event.page_range),\n",
    "                str(event.context_chunk_id) if event.context_chunk_id else None,\n",
    "                event.event_type,\n",
    "                event.event_date.isoformat() if event.event_date else None,\n",
    "                event.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        for person_name in event_data.person_names:\n",
    "            key = person_name.strip().lower()\n",
    "            person_id = person_lookup.get(key)\n",
    "            if person_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO event_participants (event_id, person_id, role) VALUES (?, ?, ?)\",\n",
    "                    (str(event.id), person_id, None),\n",
    "                )\n",
    "        for location_name in event_data.location_names:\n",
    "            key = location_name.strip().lower()\n",
    "            location_id = location_lookup.get(key)\n",
    "            if location_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO event_venues (event_id, location_id) VALUES (?, ?)\",\n",
    "                    (str(event.id), location_id),\n",
    "                )\n",
    "\n",
    "    for milestone_data in extraction.milestones:\n",
    "        person_id = person_lookup.get(milestone_data.person_name.strip().lower())\n",
    "        if not person_id:\n",
    "            continue\n",
    "        milestone = Milestone(\n",
    "            person_id=person_id,\n",
    "            artifact_id=artifact.id,\n",
    "            milestone_type=milestone_data.milestone_type,\n",
    "            milestone_date=_maybe_date(milestone_data.milestone_date),\n",
    "            description=milestone_data.description,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO milestones (id, person_id, artifact_id, milestone_type, milestone_date, description, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(milestone.id),\n",
    "                str(milestone.person_id),\n",
    "                str(milestone.artifact_id),\n",
    "                milestone.milestone_type,\n",
    "                milestone.milestone_date.isoformat() if milestone.milestone_date else None,\n",
    "                milestone.description,\n",
    "                milestone.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        if milestone_data.location_name:\n",
    "            location_id = location_lookup.get(milestone_data.location_name.strip().lower())\n",
    "            if location_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO milestone_places (milestone_id, location_id) VALUES (?, ?)\",\n",
    "                    (str(milestone.id), location_id),\n",
    "                )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return {\n",
    "        \"artifact\": artifact.title,\n",
    "        \"persons\": len(person_lookup),\n",
    "        \"locations\": len(location_lookup),\n",
    "        \"context_chunks\": len(extraction.context_chunks),\n",
    "        \"events\": len(extraction.events),\n",
    "        \"milestones\": len(extraction.milestones),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artifact': 'Nikola Tesla - Test Manuscript',\n",
       " 'persons': 4,\n",
       " 'locations': 0,\n",
       " 'context_chunks': 4,\n",
       " 'events': 9,\n",
       " 'milestones': 8}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_report = persist_document(extraction)\n",
    "ingest_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mstop\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "try:\n",
    "    from exa_py import Exa\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install exa_py to run EXA enrichment: pip install exa_py\") from exc\n",
    "\n",
    "EXA_API_KEY = os.getenv(\"EXA_API_KEY\")\n",
    "if not EXA_API_KEY:\n",
    "    raise RuntimeError(\"Missing EXA_API_KEY environment variable for EXA lookups.\")\n",
    "\n",
    "exa_client = Exa(EXA_API_KEY)\n",
    "\n",
    "conn = artifact_db.get_connection()\n",
    "locations = conn.execute(\n",
    "    \"SELECT id, name, address, latitude, longitude FROM locations ORDER BY created_at ASC\"\n",
    ").fetchall()\n",
    "\n",
    "def _get_value(obj, key, default=None):\n",
    "    if isinstance(obj, dict):\n",
    "        return obj.get(key, default)\n",
    "    return getattr(obj, key, default)\n",
    "\n",
    "def _extract_coordinates(text: str) -> Optional[Tuple[float, float]]:\n",
    "    if not text:\n",
    "        return None\n",
    "    normalized = text.replace(\"Â°\", \" \")\n",
    "    labeled = re.search(\n",
    "        r\"latitude\\s*[:=]?\\s*(-?\\d{1,2}(?:\\.\\d+)?)\\D{0,30}longitude\\s*[:=]?\\s*(-?\\d{1,3}(?:\\.\\d+)?)\",\n",
    "        normalized,\n",
    "        flags=re.IGNORECASE | re.DOTALL,\n",
    "    )\n",
    "    if labeled:\n",
    "        lat, lon = map(float, labeled.groups())\n",
    "        if abs(lat) <= 90 and abs(lon) <= 180:\n",
    "            return (lat, lon)\n",
    "    lowered = normalized.lower()\n",
    "    if \"coord\" not in lowered and \"lat\" not in lowered:\n",
    "        return None\n",
    "    generic = re.search(\n",
    "        r\"(-?\\d{1,2}(?:\\.\\d+)?)[^\\d-]{0,8}?(-?\\d{1,3}(?:\\.\\d+)?)\",\n",
    "        normalized,\n",
    "    )\n",
    "    if generic:\n",
    "        lat, lon = map(float, generic.groups())\n",
    "        if abs(lat) <= 90 and abs(lon) <= 180:\n",
    "            return (lat, lon)\n",
    "    return None\n",
    "\n",
    "def _lookup_coordinates(name: str, address: Optional[str]) -> Optional[Tuple[float, float]]:\n",
    "    query_parts = [name]\n",
    "    if address and address.strip():\n",
    "        query_parts.append(address.strip())\n",
    "    query_parts.append(\"GPS coordinates\")\n",
    "    query = \" \".join(query_parts)\n",
    "    search_response = exa_client.search(\n",
    "        query,\n",
    "        type=\"neural\",\n",
    "        use_autoprompt=True,\n",
    "        num_results=5,\n",
    "    )\n",
    "    result_ids = []\n",
    "    for result in _get_value(search_response, \"results\", []):\n",
    "        result_id = _get_value(result, \"id\")\n",
    "        if result_id:\n",
    "            result_ids.append(result_id)\n",
    "    if not result_ids:\n",
    "        return None\n",
    "    contents = exa_client.get_contents(result_ids)\n",
    "    for content in _get_value(contents, \"results\", []):\n",
    "        snippets = []\n",
    "        text = _get_value(content, \"text\") or \"\"\n",
    "        if text:\n",
    "            snippets.append(text)\n",
    "        highlights = _get_value(content, \"highlights\", []) or []\n",
    "        for highlight in highlights:\n",
    "            snippet = _get_value(highlight, \"snippet\") or \"\"\n",
    "            if snippet:\n",
    "                snippets.append(snippet)\n",
    "        coords = _extract_coordinates(\" \\n\".join(snippets))\n",
    "        if coords:\n",
    "            return coords\n",
    "    return None\n",
    "\n",
    "updates = []\n",
    "for row in locations:\n",
    "    if row[\"latitude\"] is not None and row[\"longitude\"] is not None:\n",
    "        continue\n",
    "    coords = _lookup_coordinates(row[\"name\"], row[\"address\"])\n",
    "    if not coords:\n",
    "        continue\n",
    "    conn.execute(\n",
    "        \"UPDATE locations SET latitude = ?, longitude = ? WHERE id = ?\",\n",
    "        (coords[0], coords[1], row[\"id\"]),\n",
    "    )\n",
    "    updates.append(\n",
    "        {\n",
    "            \"name\": row[\"name\"],\n",
    "            \"address\": row[\"address\"],\n",
    "            \"latitude\": coords[0],\n",
    "            \"longitude\": coords[1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "{\"enriched\": len(updates), \"details\": updates}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
