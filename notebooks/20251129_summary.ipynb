{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import json\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Ensure notebooks can import project modules.\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from Models import artifact_db\n",
    "from Models.artifact_models import (\n",
    "    Artifact,\n",
    "    DocumentExtraction,\n",
    "    Person,\n",
    "    Location,\n",
    "    ContextChunk,\n",
    "    Event,\n",
    "    Milestone,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database initialized: /Users/ryanpasecky/Historian_Assistant/Models/data/historian.db\n",
      "Reading from: /Users/ryanpasecky/Historian_Assistant/Library/test_tesla.txt\n"
     ]
    }
   ],
   "source": [
    "# Configure the artifact source and metadata.\n",
    "ARTIFACT_FILE = Path(\"../Library/test_tesla.txt\")\n",
    "MAX_CHARS = 8000  # trim super long sources for notebook experimentation\n",
    "\n",
    "ARTIFACT_TITLE = \"Nikola Tesla - Test Manuscript\"\n",
    "ARTIFACT_AUTHOR = \"Nikola Tesla\"\n",
    "\n",
    "artifact_db.init_db()\n",
    "artifact_metadata = Artifact(title=ARTIFACT_TITLE, author=ARTIFACT_AUTHOR)\n",
    "print(f\"Reading from: {ARTIFACT_FILE.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_JSON = json.dumps(DocumentExtraction.model_json_schema(), indent=2)\n",
    "DOCUMENT_EXTRACTION_INSTRUCTIONS = \"\"\"You are a historian data assistant. Read the artifact text and extract structured data.\n",
    "Please follow these guidelines:\n",
    "- Capture a concise artifact summary plus key people, locations (with address + coordinates when present), events, and milestones.\n",
    "- Map page numbers to `[start, end]` ranges where possible.\n",
    "- Use ISO dates (`YYYY-MM-DD`) when you can infer an exact day. Otherwise provide the most precise partial you can (e.g., `1919`, `1903-05`).\n",
    "- Person and location names should stay consistent so they can be re-linked later.\n",
    "- Only include milestone or event participants when the text clearly states their involvement.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_artifact_text(path: Path, max_chars: int) -> str:\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    return text[:max_chars]\n",
    "\n",
    "\n",
    "def build_prompt(artifact: Artifact, text: str) -> str:\n",
    "    return f\"\"\"{DOCUMENT_EXTRACTION_INSTRUCTIONS}\n",
    "\n",
    "Artifact metadata:\n",
    "{json.dumps(artifact.model_dump(), indent=2, default=str)}\n",
    "\n",
    "Target schema:\n",
    "{SCHEMA_JSON}\n",
    "\n",
    "Artifact text:\n",
    "{text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    "    max_retries=2,\n",
    ")\n",
    "structured_llm = llm.with_structured_output(DocumentExtraction)\n",
    "print(\"LLM ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = load_artifact_text(ARTIFACT_FILE, MAX_CHARS)\n",
    "prompt = build_prompt(artifact_metadata, raw_text)\n",
    "extraction = structured_llm.invoke(prompt)\n",
    "# enforce trusted artifact metadata so downstream IDs remain stable\n",
    "extraction = extraction.model_copy(update={\"artifact\": artifact_metadata})\n",
    "extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Artifact: {extraction.artifact.title} ({extraction.artifact.author})\")\n",
    "print(f\"People: {len(extraction.persons)} | Locations: {len(extraction.locations)} | Events: {len(extraction.events)} | Milestones: {len(extraction.milestones)}\")\n",
    "if extraction.context_chunks:\n",
    "    print(f\"Context chunks: {len(extraction.context_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json_or_null(value):\n",
    "    if value in (None, [], {}):\n",
    "        return None\n",
    "    return json.dumps(value)\n",
    "\n",
    "\n",
    "def _maybe_date(value: str | None):\n",
    "    if not value:\n",
    "        return None\n",
    "    try:\n",
    "        return date.fromisoformat(value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def persist_document(extraction: DocumentExtraction) -> Dict[str, int]:\n",
    "    conn = artifact_db.get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    artifact = extraction.artifact\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT OR REPLACE INTO artifacts (id, title, author, publication_year, time_period_start, time_period_end, created_at)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (\n",
    "            str(artifact.id),\n",
    "            artifact.title,\n",
    "            artifact.author,\n",
    "            artifact.publication_year,\n",
    "            artifact.time_period_start,\n",
    "            artifact.time_period_end,\n",
    "            artifact.created_at.isoformat(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    person_lookup: Dict[str, str] = {}\n",
    "    for person_data in extraction.persons:\n",
    "        key = person_data.name.strip().lower()\n",
    "        if not key or key in person_lookup:\n",
    "            continue\n",
    "        person = Person(\n",
    "            name=person_data.name,\n",
    "            aliases=person_data.aliases,\n",
    "            artifact_id=artifact.id,\n",
    "            birth_year=person_data.birth_year,\n",
    "            death_year=person_data.death_year,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO persons (id, name, aliases, artifact_id, birth_year, death_year, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(person.id),\n",
    "                person.name,\n",
    "                _json_or_null(person.aliases),\n",
    "                str(person.artifact_id),\n",
    "                person.birth_year,\n",
    "                person.death_year,\n",
    "                person.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        person_lookup[key] = str(person.id)\n",
    "\n",
    "    location_lookup: Dict[str, str] = {}\n",
    "    for location_data in extraction.locations:\n",
    "        key = location_data.name.strip().lower()\n",
    "        if not key or key in location_lookup:\n",
    "            continue\n",
    "        location = Location(\n",
    "            name=location_data.name,\n",
    "            aliases=location_data.aliases,\n",
    "            artifact_id=artifact.id,\n",
    "            address=location_data.address,\n",
    "            latitude=location_data.latitude,\n",
    "            longitude=location_data.longitude,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO locations (id, name, aliases, artifact_id, address, latitude, longitude, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(location.id),\n",
    "                location.name,\n",
    "                _json_or_null(location.aliases),\n",
    "                str(location.artifact_id),\n",
    "                location.address,\n",
    "                location.latitude,\n",
    "                location.longitude,\n",
    "                location.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        location_lookup[key] = str(location.id)\n",
    "\n",
    "    chunk_lookup: Dict[str, str] = {}\n",
    "    for chunk_data in extraction.context_chunks:\n",
    "        chunk = ContextChunk(\n",
    "            artifact_id=artifact.id,\n",
    "            chunk_label=chunk_data.chunk_label,\n",
    "            page_range=chunk_data.page_range,\n",
    "            summary=chunk_data.summary,\n",
    "            key_persons=chunk_data.key_persons,\n",
    "            key_locations=chunk_data.key_locations,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO context_chunks (id, artifact_id, chunk_label, page_range, summary, key_persons, key_locations, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(chunk.id),\n",
    "                str(chunk.artifact_id),\n",
    "                chunk.chunk_label,\n",
    "                _json_or_null(chunk.page_range),\n",
    "                chunk.summary,\n",
    "                _json_or_null(chunk.key_persons),\n",
    "                _json_or_null(chunk.key_locations),\n",
    "                chunk.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        if chunk.chunk_label:\n",
    "            chunk_lookup[chunk.chunk_label.strip().lower()] = str(chunk.id)\n",
    "\n",
    "    for event_data in extraction.events:\n",
    "        context_chunk_id = None\n",
    "        if event_data.context_label:\n",
    "            context_chunk_id = chunk_lookup.get(event_data.context_label.strip().lower())\n",
    "        event = Event(\n",
    "            description=event_data.description,\n",
    "            artifact_id=artifact.id,\n",
    "            page_range=event_data.page_range,\n",
    "            context_chunk_id=context_chunk_id,\n",
    "            event_type=event_data.event_type,\n",
    "            event_date=_maybe_date(event_data.event_date),\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO events (id, description, artifact_id, page_range, context_chunk_id, event_type, event_date, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(event.id),\n",
    "                event.description,\n",
    "                str(event.artifact_id),\n",
    "                _json_or_null(event.page_range),\n",
    "                str(event.context_chunk_id) if event.context_chunk_id else None,\n",
    "                event.event_type,\n",
    "                event.event_date.isoformat() if event.event_date else None,\n",
    "                event.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        for person_name in event_data.person_names:\n",
    "            key = person_name.strip().lower()\n",
    "            person_id = person_lookup.get(key)\n",
    "            if person_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO event_participants (event_id, person_id, role) VALUES (?, ?, ?)\",\n",
    "                    (str(event.id), person_id, None),\n",
    "                )\n",
    "        for location_name in event_data.location_names:\n",
    "            key = location_name.strip().lower()\n",
    "            location_id = location_lookup.get(key)\n",
    "            if location_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO event_venues (event_id, location_id) VALUES (?, ?)\",\n",
    "                    (str(event.id), location_id),\n",
    "                )\n",
    "\n",
    "    for milestone_data in extraction.milestones:\n",
    "        person_id = person_lookup.get(milestone_data.person_name.strip().lower())\n",
    "        if not person_id:\n",
    "            continue\n",
    "        milestone = Milestone(\n",
    "            person_id=person_id,\n",
    "            artifact_id=artifact.id,\n",
    "            milestone_type=milestone_data.milestone_type,\n",
    "            milestone_date=_maybe_date(milestone_data.milestone_date),\n",
    "            description=milestone_data.description,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO milestones (id, person_id, artifact_id, milestone_type, milestone_date, description, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(milestone.id),\n",
    "                str(milestone.person_id),\n",
    "                str(milestone.artifact_id),\n",
    "                milestone.milestone_type,\n",
    "                milestone.milestone_date.isoformat() if milestone.milestone_date else None,\n",
    "                milestone.description,\n",
    "                milestone.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        if milestone_data.location_name:\n",
    "            location_id = location_lookup.get(milestone_data.location_name.strip().lower())\n",
    "            if location_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO milestone_places (milestone_id, location_id) VALUES (?, ?)\",\n",
    "                    (str(milestone.id), location_id),\n",
    "                )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return {\n",
    "        \"artifact\": artifact.title,\n",
    "        \"persons\": len(person_lookup),\n",
    "        \"locations\": len(location_lookup),\n",
    "        \"context_chunks\": len(extraction.context_chunks),\n",
    "        \"events\": len(extraction.events),\n",
    "        \"milestones\": len(extraction.milestones),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_report = persist_document(extraction)\n",
    "ingest_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
