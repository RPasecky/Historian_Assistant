{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "from datetime import date, datetime\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Ensure notebooks can import project modules.\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from Models import artifact_db\n",
    "from Models.artifact_models import (\n",
    "    Artifact,\n",
    "    DocumentExtraction,\n",
    "    Person,\n",
    "    Location,\n",
    "    ContextChunk,\n",
    "    Event,\n",
    "    Milestone,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the artifact source and metadata.\n",
    "ARTIFACT_FILE = Path(\"../Library/data.txt\")\n",
    "MAX_CHARS = 8000  # trim super long sources for notebook experimentation\n",
    "\n",
    "ARTIFACT_TITLE = \"Nikola Tesla - Test Manuscript\"\n",
    "ARTIFACT_AUTHOR = \"Nikola Tesla\"\n",
    "\n",
    "artifact_db.init_db()\n",
    "artifact_metadata = Artifact(title=ARTIFACT_TITLE, author=ARTIFACT_AUTHOR)\n",
    "print(f\"Reading from: {ARTIFACT_FILE.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_JSON = json.dumps(DocumentExtraction.model_json_schema(), indent=2)\n",
    "DOCUMENT_EXTRACTION_INSTRUCTIONS = \"\"\"You are a historian data assistant. Read the artifact text and extract structured data.\n",
    "Please follow these guidelines:\n",
    "- Capture a concise artifact summary of all the key people, locations (with address + coordinates when present), events, and milestones.\n",
    "- Map page numbers to `[start, end]` ranges where possible.\n",
    "- Use ISO dates (`YYYY-MM-DD`) when you can infer an exact day. Otherwise provide the most precise partial you can (e.g., `1919`, `1903-05`).\n",
    "- Person and location names should stay consistent so they can be re-linked later.\n",
    "- Only include milestone or event participants when the text clearly states their involvement.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_artifact_text(path: Path, max_chars: int) -> str:\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    return text[:max_chars]\n",
    "\n",
    "\n",
    "def build_prompt(artifact: Artifact, text: str) -> str:\n",
    "    return f\"\"\"{DOCUMENT_EXTRACTION_INSTRUCTIONS}\n",
    "\n",
    "Artifact metadata:\n",
    "{json.dumps(artifact.model_dump(), indent=2, default=str)}\n",
    "\n",
    "Target schema:\n",
    "{SCHEMA_JSON}\n",
    "\n",
    "Artifact text:\n",
    "{text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0.0,\n",
    "    max_retries=2,\n",
    ")\n",
    "structured_llm = llm.with_structured_output(DocumentExtraction)\n",
    "print(\"LLM ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = load_artifact_text(ARTIFACT_FILE, MAX_CHARS)\n",
    "prompt = build_prompt(artifact_metadata, raw_text)\n",
    "extraction = structured_llm.invoke(prompt)\n",
    "# enforce trusted artifact metadata so downstream IDs remain stable\n",
    "extraction = extraction.model_copy(update={\"artifact\": artifact_metadata})\n",
    "extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Artifact: {extraction.artifact.title} ({extraction.artifact.author})\")\n",
    "print(f\"People: {len(extraction.persons)} | Locations: {len(extraction.locations)} | Events: {len(extraction.events)} | Milestones: {len(extraction.milestones)}\")\n",
    "if extraction.context_chunks:\n",
    "    print(f\"Context chunks: {len(extraction.context_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json_or_null(value):\n",
    "    if value in (None, [], {}):\n",
    "        return None\n",
    "    return json.dumps(value)\n",
    "\n",
    "\n",
    "def _maybe_date(value: str | None):\n",
    "    if not value:\n",
    "        return None\n",
    "    cleaned = value.strip()\n",
    "    if not cleaned:\n",
    "        return None\n",
    "    normalized = cleaned.replace(\"XX\", \"01\")\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y-%m\", \"%Y\"):\n",
    "        try:\n",
    "            parsed = datetime.strptime(normalized, fmt)\n",
    "            if fmt == \"%Y\":\n",
    "                return date(parsed.year, 1, 1)\n",
    "            if fmt == \"%Y-%m\":\n",
    "                return date(parsed.year, parsed.month, 1)\n",
    "            return parsed.date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    match = re.search(r\"(\\d{4})\", cleaned)\n",
    "    if match:\n",
    "        return date(int(match.group(1)), 1, 1)\n",
    "    return None\n",
    "\n",
    "def persist_document(extraction: DocumentExtraction) -> Dict[str, int]:\n",
    "    conn = artifact_db.get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    artifact = extraction.artifact\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT OR REPLACE INTO artifacts (id, title, author, publication_year, time_period_start, time_period_end, created_at)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (\n",
    "            str(artifact.id),\n",
    "            artifact.title,\n",
    "            artifact.author,\n",
    "            artifact.publication_year,\n",
    "            artifact.time_period_start,\n",
    "            artifact.time_period_end,\n",
    "            artifact.created_at.isoformat(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    person_lookup: Dict[str, str] = {}\n",
    "    for person_data in extraction.persons:\n",
    "        key = person_data.name.strip().lower()\n",
    "        if not key or key in person_lookup:\n",
    "            continue\n",
    "        person = Person(\n",
    "            name=person_data.name,\n",
    "            aliases=person_data.aliases,\n",
    "            artifact_id=artifact.id,\n",
    "            birth_year=person_data.birth_year,\n",
    "            death_year=person_data.death_year,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO persons (id, name, aliases, artifact_id, birth_year, death_year, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(person.id),\n",
    "                person.name,\n",
    "                _json_or_null(person.aliases),\n",
    "                str(person.artifact_id),\n",
    "                person.birth_year,\n",
    "                person.death_year,\n",
    "                person.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        person_lookup[key] = str(person.id)\n",
    "\n",
    "    location_lookup: Dict[str, str] = {}\n",
    "    for location_data in extraction.locations:\n",
    "        key = location_data.name.strip().lower()\n",
    "        if not key or key in location_lookup:\n",
    "            continue\n",
    "        location = Location(\n",
    "            name=location_data.name,\n",
    "            aliases=location_data.aliases,\n",
    "            artifact_id=artifact.id,\n",
    "            address=location_data.address,\n",
    "            latitude=location_data.latitude,\n",
    "            longitude=location_data.longitude,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO locations (id, name, aliases, artifact_id, address, latitude, longitude, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(location.id),\n",
    "                location.name,\n",
    "                _json_or_null(location.aliases),\n",
    "                str(location.artifact_id),\n",
    "                location.address,\n",
    "                location.latitude,\n",
    "                location.longitude,\n",
    "                location.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        location_lookup[key] = str(location.id)\n",
    "\n",
    "    chunk_lookup: Dict[str, str] = {}\n",
    "    for chunk_data in extraction.context_chunks:\n",
    "        chunk = ContextChunk(\n",
    "            artifact_id=artifact.id,\n",
    "            chunk_label=chunk_data.chunk_label,\n",
    "            page_range=chunk_data.page_range,\n",
    "            summary=chunk_data.summary,\n",
    "            key_persons=chunk_data.key_persons,\n",
    "            key_locations=chunk_data.key_locations,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO context_chunks (id, artifact_id, chunk_label, page_range, summary, key_persons, key_locations, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(chunk.id),\n",
    "                str(chunk.artifact_id),\n",
    "                chunk.chunk_label,\n",
    "                _json_or_null(chunk.page_range),\n",
    "                chunk.summary,\n",
    "                _json_or_null(chunk.key_persons),\n",
    "                _json_or_null(chunk.key_locations),\n",
    "                chunk.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        if chunk.chunk_label:\n",
    "            chunk_lookup[chunk.chunk_label.strip().lower()] = str(chunk.id)\n",
    "\n",
    "    for event_data in extraction.events:\n",
    "        context_chunk_id = None\n",
    "        if event_data.context_label:\n",
    "            context_chunk_id = chunk_lookup.get(event_data.context_label.strip().lower())\n",
    "        event = Event(\n",
    "            description=event_data.description,\n",
    "            artifact_id=artifact.id,\n",
    "            page_range=event_data.page_range,\n",
    "            context_chunk_id=context_chunk_id,\n",
    "            event_type=event_data.event_type,\n",
    "            event_date=_maybe_date(event_data.event_date),\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO events (id, description, artifact_id, page_range, context_chunk_id, event_type, event_date, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(event.id),\n",
    "                event.description,\n",
    "                str(event.artifact_id),\n",
    "                _json_or_null(event.page_range),\n",
    "                str(event.context_chunk_id) if event.context_chunk_id else None,\n",
    "                event.event_type,\n",
    "                event.event_date.isoformat() if event.event_date else None,\n",
    "                event.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        for person_name in event_data.person_names:\n",
    "            key = person_name.strip().lower()\n",
    "            person_id = person_lookup.get(key)\n",
    "            if person_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO event_participants (event_id, person_id, role) VALUES (?, ?, ?)\",\n",
    "                    (str(event.id), person_id, None),\n",
    "                )\n",
    "        for location_name in event_data.location_names:\n",
    "            key = location_name.strip().lower()\n",
    "            location_id = location_lookup.get(key)\n",
    "            if location_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO event_venues (event_id, location_id) VALUES (?, ?)\",\n",
    "                    (str(event.id), location_id),\n",
    "                )\n",
    "\n",
    "    for milestone_data in extraction.milestones:\n",
    "        person_id = person_lookup.get(milestone_data.person_name.strip().lower())\n",
    "        if not person_id:\n",
    "            continue\n",
    "        milestone = Milestone(\n",
    "            person_id=person_id,\n",
    "            artifact_id=artifact.id,\n",
    "            milestone_type=milestone_data.milestone_type,\n",
    "            milestone_date=_maybe_date(milestone_data.milestone_date),\n",
    "            description=milestone_data.description,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO milestones (id, person_id, artifact_id, milestone_type, milestone_date, description, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(milestone.id),\n",
    "                str(milestone.person_id),\n",
    "                str(milestone.artifact_id),\n",
    "                milestone.milestone_type,\n",
    "                milestone.milestone_date.isoformat() if milestone.milestone_date else None,\n",
    "                milestone.description,\n",
    "                milestone.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        if milestone_data.location_name:\n",
    "            location_id = location_lookup.get(milestone_data.location_name.strip().lower())\n",
    "            if location_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO milestone_places (milestone_id, location_id) VALUES (?, ?)\",\n",
    "                    (str(milestone.id), location_id),\n",
    "                )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return {\n",
    "        \"artifact\": artifact.title,\n",
    "        \"persons\": len(person_lookup),\n",
    "        \"locations\": len(location_lookup),\n",
    "        \"context_chunks\": len(extraction.context_chunks),\n",
    "        \"events\": len(extraction.events),\n",
    "        \"milestones\": len(extraction.milestones),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_report = persist_document(extraction)\n",
    "ingest_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "try:\n",
    "    from exa_py import Exa\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install exa_py to run EXA enrichment: pip install exa_py\") from exc\n",
    "\n",
    "EXA_API_KEY = os.getenv(\"EXA_API_KEY\")\n",
    "if not EXA_API_KEY:\n",
    "    raise RuntimeError(\"Missing EXA_API_KEY environment variable for EXA lookups.\")\n",
    "\n",
    "exa_client = Exa(EXA_API_KEY)\n",
    "\n",
    "conn = artifact_db.get_connection()\n",
    "locations = conn.execute(\n",
    "    \"SELECT id, name, address, latitude, longitude FROM locations ORDER BY created_at ASC\"\n",
    ").fetchall()\n",
    "\n",
    "def _get_value(obj, key, default=None):\n",
    "    if isinstance(obj, dict):\n",
    "        return obj.get(key, default)\n",
    "    return getattr(obj, key, default)\n",
    "\n",
    "def _extract_coordinates(text: str) -> Optional[Tuple[float, float]]:\n",
    "    if not text:\n",
    "        return None\n",
    "    normalized = text.replace(\"Â°\", \" \")\n",
    "    labeled = re.search(\n",
    "        r\"latitude\\s*[:=]?\\s*(-?\\d{1,2}(?:\\.\\d+)?)\\D{0,30}longitude\\s*[:=]?\\s*(-?\\d{1,3}(?:\\.\\d+)?)\",\n",
    "        normalized,\n",
    "        flags=re.IGNORECASE | re.DOTALL,\n",
    "    )\n",
    "    if labeled:\n",
    "        lat, lon = map(float, labeled.groups())\n",
    "        if abs(lat) <= 90 and abs(lon) <= 180:\n",
    "            return (lat, lon)\n",
    "    lowered = normalized.lower()\n",
    "    if \"coord\" not in lowered and \"lat\" not in lowered:\n",
    "        return None\n",
    "    generic = re.search(\n",
    "        r\"(-?\\d{1,2}(?:\\.\\d+)?)[^\\d-]{0,8}?(-?\\d{1,3}(?:\\.\\d+)?)\",\n",
    "        normalized,\n",
    "    )\n",
    "    if generic:\n",
    "        lat, lon = map(float, generic.groups())\n",
    "        if abs(lat) <= 90 and abs(lon) <= 180:\n",
    "            return (lat, lon)\n",
    "    return None\n",
    "\n",
    "def _lookup_coordinates(name: str, address: Optional[str]) -> Optional[Tuple[float, float]]:\n",
    "    query_parts = [name]\n",
    "    if address and address.strip():\n",
    "        query_parts.append(address.strip())\n",
    "    query_parts.append(\"GPS coordinates\")\n",
    "    query = \" \".join(query_parts)\n",
    "    search_response = exa_client.search(\n",
    "        query,\n",
    "        type=\"neural\",\n",
    "        use_autoprompt=True,\n",
    "        num_results=5,\n",
    "    )\n",
    "    result_ids = []\n",
    "    for result in _get_value(search_response, \"results\", []):\n",
    "        result_id = _get_value(result, \"id\")\n",
    "        if result_id:\n",
    "            result_ids.append(result_id)\n",
    "    if not result_ids:\n",
    "        return None\n",
    "    contents = exa_client.get_contents(result_ids)\n",
    "    for content in _get_value(contents, \"results\", []):\n",
    "        snippets = []\n",
    "        text = _get_value(content, \"text\") or \"\"\n",
    "        if text:\n",
    "            snippets.append(text)\n",
    "        highlights = _get_value(content, \"highlights\", []) or []\n",
    "        for highlight in highlights:\n",
    "            snippet = _get_value(highlight, \"snippet\") or \"\"\n",
    "            if snippet:\n",
    "                snippets.append(snippet)\n",
    "        coords = _extract_coordinates(\" \\n\".join(snippets))\n",
    "        if coords:\n",
    "            return coords\n",
    "    return None\n",
    "\n",
    "updates = []\n",
    "for row in locations:\n",
    "    if row[\"latitude\"] is not None and row[\"longitude\"] is not None:\n",
    "        continue\n",
    "    coords = _lookup_coordinates(row[\"name\"], row[\"address\"])\n",
    "    if not coords:\n",
    "        continue\n",
    "    conn.execute(\n",
    "        \"UPDATE locations SET latitude = ?, longitude = ? WHERE id = ?\",\n",
    "        (coords[0], coords[1], row[\"id\"]),\n",
    "    )\n",
    "    updates.append(\n",
    "        {\n",
    "            \"name\": row[\"name\"],\n",
    "            \"address\": row[\"address\"],\n",
    "            \"latitude\": coords[0],\n",
    "            \"longitude\": coords[1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "{\"enriched\": len(updates), \"details\": updates}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
