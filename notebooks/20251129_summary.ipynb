{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "from datetime import date, datetime\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Ensure notebooks can import project modules.\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from Models import artifact_db\n",
    "from Models.artifact_models import (\n",
    "    Artifact,\n",
    "    DocumentExtraction,\n",
    "    Person,\n",
    "    Location,\n",
    "    ContextChunk,\n",
    "    Event,\n",
    "    Milestone,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database initialized: /Users/dirk/Historian_Assistant/Models/data/historian.db\n",
      "Reading from: /Users/dirk/Historian_Assistant/Library/data_patents1.txt\n"
     ]
    }
   ],
   "source": [
    "# Configure the artifact source and metadata.\n",
    "ARTIFACT_FILE = Path(\"../Library/data.txt\")\n",
    "MAX_CHARS = 8000  # trim super long sources for notebook experimentation\n",
    "\n",
    "ARTIFACT_TITLE = \"Nikola Tesla - Test Manuscript\"\n",
    "ARTIFACT_AUTHOR = \"Nikola Tesla\"\n",
    "\n",
    "artifact_db.init_db()\n",
    "artifact_metadata = Artifact(title=ARTIFACT_TITLE, author=ARTIFACT_AUTHOR)\n",
    "print(f\"Reading from: {ARTIFACT_FILE.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_JSON = json.dumps(DocumentExtraction.model_json_schema(), indent=2)\n",
    "DOCUMENT_EXTRACTION_INSTRUCTIONS = \"\"\"You are a historian data assistant. Read the artifact text and extract structured data.\n",
    "Please follow these guidelines:\n",
    "- Capture a concise artifact summary plus key people, locations (with address + coordinates when present), events, and milestones.\n",
    "- Map page numbers to `[start, end]` ranges where possible.\n",
    "- Use ISO dates (`YYYY-MM-DD`) when you can infer an exact day. Otherwise provide the most precise partial you can (e.g., `1919`, `1903-05`).\n",
    "- Person and location names should stay consistent so they can be re-linked later.\n",
    "- Only include milestone or event participants when the text clearly states their involvement.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_artifact_text(path: Path, max_chars: int) -> str:\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    return text[:max_chars]\n",
    "\n",
    "\n",
    "def build_prompt(artifact: Artifact, text: str) -> str:\n",
    "    return f\"\"\"{DOCUMENT_EXTRACTION_INSTRUCTIONS}\n",
    "\n",
    "Artifact metadata:\n",
    "{json.dumps(artifact.model_dump(), indent=2, default=str)}\n",
    "\n",
    "Target schema:\n",
    "{SCHEMA_JSON}\n",
    "\n",
    "Artifact text:\n",
    "{text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    "    max_retries=2,\n",
    ")\n",
    "structured_llm = llm.with_structured_output(DocumentExtraction)\n",
    "print(\"LLM ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentExtraction(artifact=Artifact(id=UUID('d53da09d-91b2-444d-b7bb-c34f79d5d453'), title='Nikola Tesla - Test Manuscript', author='Nikola Tesla', publication_year=None, time_period_start=None, time_period_end=None, created_at=datetime.datetime(2025, 11, 29, 15, 39, 23, 631340)), persons=[PersonExtraction(name='Nikola Tesla', aliases=[], birth_year=None, death_year=None), PersonExtraction(name='Mr. William B. Rankine', aliases=[], birth_year=None, death_year=None), PersonExtraction(name='Mr. Morgan', aliases=[], birth_year=None, death_year=None), PersonExtraction(name='Colonel Astor', aliases=[], birth_year=None, death_year=None)], locations=[], context_chunks=[ContextChunkExtraction(chunk_label='SECTION 1: PROFESSIONAL ANNOUNCEMENT', page_range=[], summary='Nikola Tesla announces his availability for professional services as a consulting electrician and engineer, anticipating revolutionary advances in energy, transportation, and communication, driven by high-potential and high-frequency currents and regenerative refrigeration. He aims to contribute his knowledge and experience to these developments.', key_persons=['Nikola Tesla'], key_locations=['Long Island, N. Y.', 'Waldorf, New York City']), ContextChunkExtraction(chunk_label='SECTION 2: EXCERPTS ON WIRELESS TRANSMISSION', page_range=[], summary=\"Excerpts from Nikola Tesla's lectures and writings on wireless transmission of energy and signals. He discusses the practicability of transmitting signals and power without wires, using the earth as a conductor, and his discovery of stationary electrical waves, which provides absolute certainty for global wireless communication. He also mentions related U.S. Patents.\", key_persons=['Nikola Tesla'], key_locations=['Philadelphia', 'St. Louis']), ContextChunkExtraction(chunk_label='SECTION 3: CORRESPONDENCE', page_range=[], summary=\"A letter from Nikola Tesla to William B. Rankine discussing the financial status of The Nikola Tesla Company, patent assignments to Mr. Morgan and Colonel Astor, and the delayed formation of 'Tesla Electric & Manufacturing Company.' Tesla expresses confidence in the Wardenclyffe plant's potential for global telegraphy and telephony and the significant value of his patents.\", key_persons=['Nikola Tesla', 'Mr. William B. Rankine', 'Mr. Morgan', 'Colonel Astor'], key_locations=['New York City', '35 Wall Street, New York City', 'Wardenclyffe'])], events=[EventExtraction(description='Nikola Tesla announces his professional services as a consulting electrician and engineer.', page_range=[], event_type=None, context_label='SECTION 1: PROFESSIONAL ANNOUNCEMENT', event_date='1904-01-01', quotes=[], interactions=[], person_names=['Nikola Tesla'], location_names=['Long Island, N. Y.', 'Waldorf, New York City']), EventExtraction(description=\"Nikola Tesla delivers a lecture 'Light and Other High-frequency Phenomena' before the Franklin Institute.\", page_range=[], event_type=None, context_label='SECTION 2: EXCERPTS ON WIRELESS TRANSMISSION', event_date='1893-02', quotes=[], interactions=[], person_names=['Nikola Tesla'], location_names=['Philadelphia']), EventExtraction(description=\"Nikola Tesla delivers a lecture 'Light and Other High-frequency Phenomena' before the National Electric Light Association.\", page_range=[], event_type=None, context_label='SECTION 2: EXCERPTS ON WIRELESS TRANSMISSION', event_date='1893-03', quotes=[], interactions=[], person_names=['Nikola Tesla'], location_names=['St. Louis']), EventExtraction(description=\"Nikola Tesla publishes 'The Problem of Increasing Human Energy' in Century magazine.\", page_range=[], event_type=None, context_label='SECTION 2: EXCERPTS ON WIRELESS TRANSMISSION', event_date='1900-06', quotes=[], interactions=[], person_names=['Nikola Tesla'], location_names=[]), EventExtraction(description='Nikola Tesla makes a personal agreement with Mr. Morgan assigning a part of some patents relating to telegraphy and lighting.', page_range=[], event_type=None, context_label='SECTION 3: CORRESPONDENCE', event_date='1904-04-19', quotes=[], interactions=[], person_names=['Nikola Tesla', 'Mr. Morgan'], location_names=[]), EventExtraction(description=\"Colonel Astor's interest in Nikola Tesla's inventions is adjusted to include all inventions instead of two.\", page_range=[], event_type=None, context_label='SECTION 3: CORRESPONDENCE', event_date='1904-04-19', quotes=[], interactions=[], person_names=['Colonel Astor'], location_names=[]), EventExtraction(description=\"An attempt was made to form a manufacturing company under the name 'Tesla Electric & Manufacturing Company' with a capital of $5,000,000, but it was delayed due to unfavorable conditions.\", page_range=[], event_type=None, context_label='SECTION 3: CORRESPONDENCE', event_date='1903-06', quotes=[], interactions=[], person_names=['Nikola Tesla'], location_names=[])], milestones=[MilestoneExtraction(person_name='Nikola Tesla', milestone_type='Discovery', milestone_date='1900-06', description='Discovery of stationary electrical waves, providing absolute certitude for wireless communication.', location_name=None), MilestoneExtraction(person_name='Nikola Tesla', milestone_type='Project Development', milestone_date='1904', description='Plan to complete the plant at Wardenclyffe to enable global telegraphy and telephony.', location_name='Wardenclyffe')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = load_artifact_text(ARTIFACT_FILE, MAX_CHARS)\n",
    "prompt = build_prompt(artifact_metadata, raw_text)\n",
    "extraction = structured_llm.invoke(prompt)\n",
    "# enforce trusted artifact metadata so downstream IDs remain stable\n",
    "extraction = extraction.model_copy(update={\"artifact\": artifact_metadata})\n",
    "extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact: Nikola Tesla - Test Manuscript (Nikola Tesla)\n",
      "People: 4 | Locations: 0 | Events: 7 | Milestones: 2\n",
      "Context chunks: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Artifact: {extraction.artifact.title} ({extraction.artifact.author})\")\n",
    "print(f\"People: {len(extraction.persons)} | Locations: {len(extraction.locations)} | Events: {len(extraction.events)} | Milestones: {len(extraction.milestones)}\")\n",
    "if extraction.context_chunks:\n",
    "    print(f\"Context chunks: {len(extraction.context_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json_or_null(value):\n",
    "    if value in (None, [], {}):\n",
    "        return None\n",
    "    return json.dumps(value)\n",
    "\n",
    "\n",
    "def _maybe_date(value: str | None):\n",
    "    if not value:\n",
    "        return None\n",
    "    cleaned = value.strip()\n",
    "    if not cleaned:\n",
    "        return None\n",
    "    normalized = cleaned.replace(\"XX\", \"01\")\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y-%m\", \"%Y\"):\n",
    "        try:\n",
    "            parsed = datetime.strptime(normalized, fmt)\n",
    "            if fmt == \"%Y\":\n",
    "                return date(parsed.year, 1, 1)\n",
    "            if fmt == \"%Y-%m\":\n",
    "                return date(parsed.year, parsed.month, 1)\n",
    "            return parsed.date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    match = re.search(r\"(\\d{4})\", cleaned)\n",
    "    if match:\n",
    "        return date(int(match.group(1)), 1, 1)\n",
    "    return None\n",
    "\n",
    "def persist_document(extraction: DocumentExtraction) -> Dict[str, int]:\n",
    "    conn = artifact_db.get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    artifact = extraction.artifact\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT OR REPLACE INTO artifacts (id, title, author, publication_year, time_period_start, time_period_end, created_at)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (\n",
    "            str(artifact.id),\n",
    "            artifact.title,\n",
    "            artifact.author,\n",
    "            artifact.publication_year,\n",
    "            artifact.time_period_start,\n",
    "            artifact.time_period_end,\n",
    "            artifact.created_at.isoformat(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    person_lookup: Dict[str, str] = {}\n",
    "    for person_data in extraction.persons:\n",
    "        key = person_data.name.strip().lower()\n",
    "        if not key or key in person_lookup:\n",
    "            continue\n",
    "        person = Person(\n",
    "            name=person_data.name,\n",
    "            aliases=person_data.aliases,\n",
    "            artifact_id=artifact.id,\n",
    "            birth_year=person_data.birth_year,\n",
    "            death_year=person_data.death_year,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO persons (id, name, aliases, artifact_id, birth_year, death_year, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(person.id),\n",
    "                person.name,\n",
    "                _json_or_null(person.aliases),\n",
    "                str(person.artifact_id),\n",
    "                person.birth_year,\n",
    "                person.death_year,\n",
    "                person.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        person_lookup[key] = str(person.id)\n",
    "\n",
    "    location_lookup: Dict[str, str] = {}\n",
    "    for location_data in extraction.locations:\n",
    "        key = location_data.name.strip().lower()\n",
    "        if not key or key in location_lookup:\n",
    "            continue\n",
    "        location = Location(\n",
    "            name=location_data.name,\n",
    "            aliases=location_data.aliases,\n",
    "            artifact_id=artifact.id,\n",
    "            address=location_data.address,\n",
    "            latitude=location_data.latitude,\n",
    "            longitude=location_data.longitude,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO locations (id, name, aliases, artifact_id, address, latitude, longitude, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(location.id),\n",
    "                location.name,\n",
    "                _json_or_null(location.aliases),\n",
    "                str(location.artifact_id),\n",
    "                location.address,\n",
    "                location.latitude,\n",
    "                location.longitude,\n",
    "                location.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        location_lookup[key] = str(location.id)\n",
    "\n",
    "    chunk_lookup: Dict[str, str] = {}\n",
    "    for chunk_data in extraction.context_chunks:\n",
    "        chunk = ContextChunk(\n",
    "            artifact_id=artifact.id,\n",
    "            chunk_label=chunk_data.chunk_label,\n",
    "            page_range=chunk_data.page_range,\n",
    "            summary=chunk_data.summary,\n",
    "            key_persons=chunk_data.key_persons,\n",
    "            key_locations=chunk_data.key_locations,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO context_chunks (id, artifact_id, chunk_label, page_range, summary, key_persons, key_locations, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(chunk.id),\n",
    "                str(chunk.artifact_id),\n",
    "                chunk.chunk_label,\n",
    "                _json_or_null(chunk.page_range),\n",
    "                chunk.summary,\n",
    "                _json_or_null(chunk.key_persons),\n",
    "                _json_or_null(chunk.key_locations),\n",
    "                chunk.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        if chunk.chunk_label:\n",
    "            chunk_lookup[chunk.chunk_label.strip().lower()] = str(chunk.id)\n",
    "\n",
    "    for event_data in extraction.events:\n",
    "        context_chunk_id = None\n",
    "        if event_data.context_label:\n",
    "            context_chunk_id = chunk_lookup.get(event_data.context_label.strip().lower())\n",
    "        event = Event(\n",
    "            description=event_data.description,\n",
    "            artifact_id=artifact.id,\n",
    "            page_range=event_data.page_range,\n",
    "            context_chunk_id=context_chunk_id,\n",
    "            event_type=event_data.event_type,\n",
    "            event_date=_maybe_date(event_data.event_date),\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO events (id, description, artifact_id, page_range, context_chunk_id, event_type, event_date, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(event.id),\n",
    "                event.description,\n",
    "                str(event.artifact_id),\n",
    "                _json_or_null(event.page_range),\n",
    "                str(event.context_chunk_id) if event.context_chunk_id else None,\n",
    "                event.event_type,\n",
    "                event.event_date.isoformat() if event.event_date else None,\n",
    "                event.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        for person_name in event_data.person_names:\n",
    "            key = person_name.strip().lower()\n",
    "            person_id = person_lookup.get(key)\n",
    "            if person_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO event_participants (event_id, person_id, role) VALUES (?, ?, ?)\",\n",
    "                    (str(event.id), person_id, None),\n",
    "                )\n",
    "        for location_name in event_data.location_names:\n",
    "            key = location_name.strip().lower()\n",
    "            location_id = location_lookup.get(key)\n",
    "            if location_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO event_venues (event_id, location_id) VALUES (?, ?)\",\n",
    "                    (str(event.id), location_id),\n",
    "                )\n",
    "\n",
    "    for milestone_data in extraction.milestones:\n",
    "        person_id = person_lookup.get(milestone_data.person_name.strip().lower())\n",
    "        if not person_id:\n",
    "            continue\n",
    "        milestone = Milestone(\n",
    "            person_id=person_id,\n",
    "            artifact_id=artifact.id,\n",
    "            milestone_type=milestone_data.milestone_type,\n",
    "            milestone_date=_maybe_date(milestone_data.milestone_date),\n",
    "            description=milestone_data.description,\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT OR REPLACE INTO milestones (id, person_id, artifact_id, milestone_type, milestone_date, description, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(milestone.id),\n",
    "                str(milestone.person_id),\n",
    "                str(milestone.artifact_id),\n",
    "                milestone.milestone_type,\n",
    "                milestone.milestone_date.isoformat() if milestone.milestone_date else None,\n",
    "                milestone.description,\n",
    "                milestone.created_at.isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        if milestone_data.location_name:\n",
    "            location_id = location_lookup.get(milestone_data.location_name.strip().lower())\n",
    "            if location_id:\n",
    "                cur.execute(\n",
    "                    \"INSERT OR IGNORE INTO milestone_places (milestone_id, location_id) VALUES (?, ?)\",\n",
    "                    (str(milestone.id), location_id),\n",
    "                )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return {\n",
    "        \"artifact\": artifact.title,\n",
    "        \"persons\": len(person_lookup),\n",
    "        \"locations\": len(location_lookup),\n",
    "        \"context_chunks\": len(extraction.context_chunks),\n",
    "        \"events\": len(extraction.events),\n",
    "        \"milestones\": len(extraction.milestones),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artifact': 'Nikola Tesla - Test Manuscript',\n",
       " 'persons': 4,\n",
       " 'locations': 0,\n",
       " 'context_chunks': 3,\n",
       " 'events': 7,\n",
       " 'milestones': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_report = persist_document(extraction)\n",
    "ingest_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install exa_py to run EXA enrichment: pip install exa_py",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexa_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Exa\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'exa_py'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexa_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Exa\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInstall exa_py to run EXA enrichment: pip install exa_py\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     10\u001b[39m EXA_API_KEY = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mEXA_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m EXA_API_KEY:\n",
      "\u001b[31mImportError\u001b[39m: Install exa_py to run EXA enrichment: pip install exa_py"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "try:\n",
    "    from exa_py import Exa\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install exa_py to run EXA enrichment: pip install exa_py\") from exc\n",
    "\n",
    "EXA_API_KEY = os.getenv(\"EXA_API_KEY\")\n",
    "if not EXA_API_KEY:\n",
    "    raise RuntimeError(\"Missing EXA_API_KEY environment variable for EXA lookups.\")\n",
    "\n",
    "exa_client = Exa(EXA_API_KEY)\n",
    "\n",
    "conn = artifact_db.get_connection()\n",
    "locations = conn.execute(\n",
    "    \"SELECT id, name, address, latitude, longitude FROM locations ORDER BY created_at ASC\"\n",
    ").fetchall()\n",
    "\n",
    "def _get_value(obj, key, default=None):\n",
    "    if isinstance(obj, dict):\n",
    "        return obj.get(key, default)\n",
    "    return getattr(obj, key, default)\n",
    "\n",
    "def _extract_coordinates(text: str) -> Optional[Tuple[float, float]]:\n",
    "    if not text:\n",
    "        return None\n",
    "    normalized = text.replace(\"Â°\", \" \")\n",
    "    labeled = re.search(\n",
    "        r\"latitude\\s*[:=]?\\s*(-?\\d{1,2}(?:\\.\\d+)?)\\D{0,30}longitude\\s*[:=]?\\s*(-?\\d{1,3}(?:\\.\\d+)?)\",\n",
    "        normalized,\n",
    "        flags=re.IGNORECASE | re.DOTALL,\n",
    "    )\n",
    "    if labeled:\n",
    "        lat, lon = map(float, labeled.groups())\n",
    "        if abs(lat) <= 90 and abs(lon) <= 180:\n",
    "            return (lat, lon)\n",
    "    lowered = normalized.lower()\n",
    "    if \"coord\" not in lowered and \"lat\" not in lowered:\n",
    "        return None\n",
    "    generic = re.search(\n",
    "        r\"(-?\\d{1,2}(?:\\.\\d+)?)[^\\d-]{0,8}?(-?\\d{1,3}(?:\\.\\d+)?)\",\n",
    "        normalized,\n",
    "    )\n",
    "    if generic:\n",
    "        lat, lon = map(float, generic.groups())\n",
    "        if abs(lat) <= 90 and abs(lon) <= 180:\n",
    "            return (lat, lon)\n",
    "    return None\n",
    "\n",
    "def _lookup_coordinates(name: str, address: Optional[str]) -> Optional[Tuple[float, float]]:\n",
    "    query_parts = [name]\n",
    "    if address and address.strip():\n",
    "        query_parts.append(address.strip())\n",
    "    query_parts.append(\"GPS coordinates\")\n",
    "    query = \" \".join(query_parts)\n",
    "    search_response = exa_client.search(\n",
    "        query,\n",
    "        type=\"neural\",\n",
    "        use_autoprompt=True,\n",
    "        num_results=5,\n",
    "    )\n",
    "    result_ids = []\n",
    "    for result in _get_value(search_response, \"results\", []):\n",
    "        result_id = _get_value(result, \"id\")\n",
    "        if result_id:\n",
    "            result_ids.append(result_id)\n",
    "    if not result_ids:\n",
    "        return None\n",
    "    contents = exa_client.get_contents(result_ids)\n",
    "    for content in _get_value(contents, \"results\", []):\n",
    "        snippets = []\n",
    "        text = _get_value(content, \"text\") or \"\"\n",
    "        if text:\n",
    "            snippets.append(text)\n",
    "        highlights = _get_value(content, \"highlights\", []) or []\n",
    "        for highlight in highlights:\n",
    "            snippet = _get_value(highlight, \"snippet\") or \"\"\n",
    "            if snippet:\n",
    "                snippets.append(snippet)\n",
    "        coords = _extract_coordinates(\" \\n\".join(snippets))\n",
    "        if coords:\n",
    "            return coords\n",
    "    return None\n",
    "\n",
    "updates = []\n",
    "for row in locations:\n",
    "    if row[\"latitude\"] is not None and row[\"longitude\"] is not None:\n",
    "        continue\n",
    "    coords = _lookup_coordinates(row[\"name\"], row[\"address\"])\n",
    "    if not coords:\n",
    "        continue\n",
    "    conn.execute(\n",
    "        \"UPDATE locations SET latitude = ?, longitude = ? WHERE id = ?\",\n",
    "        (coords[0], coords[1], row[\"id\"]),\n",
    "    )\n",
    "    updates.append(\n",
    "        {\n",
    "            \"name\": row[\"name\"],\n",
    "            \"address\": row[\"address\"],\n",
    "            \"latitude\": coords[0],\n",
    "            \"longitude\": coords[1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "{\"enriched\": len(updates), \"details\": updates}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
